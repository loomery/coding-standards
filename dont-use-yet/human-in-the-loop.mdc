---
description:
globs:
alwaysApply: false
---
** don't use the rules from this file yet, use the ones from .cursor/rules instead **

# Human-in-the-Loop TDD Rules

## Description
These rules ensure the human stays engaged and can provide guidance at critical decision points during Test-Driven Development. The AI should pause and explicitly ask for user feedback in these specific situations.

## Rule 1: Post-Refactoring Confirmation

### When to Apply
After completing the mandatory refactoring phase, **before moving to the next red phase** (activating the next test).

### What to Do
1. **Stop the TDD cycle**
2. **Summarize what refactorings were attempted/completed**:
   - Naming changes made
   - Mass calculations (before/after if applicable)
   - Structural improvements
   - Any refactoring opportunities that were rejected and why
3. **Explicitly ask**:
   - "Are you satisfied with these refactorings?"
   - "Would you like me to attempt any additional refactoring before moving to the next test?"
   - "Do you see any other improvements I should consider?"

### Why This Matters
- **Refactoring is subjective** - Human judgment often catches improvements AI might miss
- **Prevents accumulation of technical debt** - Catches issues early before they compound
- **Educational opportunity** - Human can suggest alternative approaches or design patterns
- **Maintains code quality standards** - Human can enforce team/project-specific conventions

### Example
```
‚úÖ Refactoring completed:
- Renamed `add` ‚Üí `sumCommaSeparatedNumbers` (naming evaluation)
- Mass remains at 38 (no structural improvements found)
- Considered extracting helper functions but would increase complexity

Are you satisfied with these refactorings, or would you like me to attempt any additional refactoring before moving to the next test?
```

## Rule 2: Failed Prediction Recovery

### When to Apply
When the **"Guessing Game" prediction fails** - the actual test result differs significantly from what was predicted.

### What to Do
1. **Stop the TDD cycle immediately**
2. **Explain the prediction failure**:
   - What was predicted (error type, expected/actual values)
   - What actually happened
   - Why the prediction was wrong (if clear)
3. **Assess the implications**:
   - Does this indicate a misunderstanding of the code?
   - Does this suggest the test or implementation has issues?
   - Is this a learning opportunity about the system behavior?
4. **Explicitly ask**:
   - "My prediction was incorrect. Should I continue with the TDD process, or would you like me to investigate this discrepancy further?"
   - "Do you want me to explain why I think my prediction failed?"
   - "Should I adjust my understanding and continue, or take a different approach?"

### Why This Matters
- **Predictions build understanding** - Failures indicate gaps in comprehension
- **Early error detection** - Unexpected behavior might reveal bugs or design issues
- **Learning opportunity** - Human can provide insights about system behavior
- **Maintains TDD discipline** - Ensures predictions remain meaningful and accurate

### Example
```
‚ùå Prediction Failed:
- Predicted: Runtime assertion error (Expected: 3, Received: 1)
- Actual: Runtime assertion error (Expected: 3, Received: NaN)
- Issue: I incorrectly assumed parseInt("1,2") would return 1, but it actually returned NaN

This suggests I misunderstood how parseInt handles comma-separated strings. Should I continue with the TDD process, or would you like me to investigate this behavior further?
```

## Integration with TDD Process

### Modified TDD Cycle
1. **Red Phase** (compilation/runtime error)
2. **Green Phase** (minimal implementation)
3. **Refactor Phase** (mandatory improvements)
4. **üõë HUMAN CHECKPOINT**: Post-refactoring confirmation
5. **Next Red Phase** (activate next test)

### Modified Guessing Game
1. **Make explicit prediction**
2. **Run test**
3. **Compare prediction vs actual**
4. **üõë HUMAN CHECKPOINT**: If prediction failed significantly
5. **Continue TDD cycle**

## Guidelines

### When NOT to Stop
- **Minor prediction details wrong** - e.g., predicted specific error message but general error type was correct
- **Trivial refactoring questions** - e.g., obvious improvements with no reasonable alternatives
- **Clear next steps** - when the path forward is unambiguous

### When TO Stop
- **Significant prediction failures** - fundamental misunderstanding of behavior
- **Refactoring uncertainty** - multiple reasonable approaches available
- **Design decisions** - choices that affect architecture or maintainability
- **Educational moments** - opportunities for human to share knowledge or preferences

## Benefits
- **Maintains human agency** in the TDD process
- **Prevents AI from making poor design decisions** in isolation
- **Creates learning opportunities** for both human and AI
- **Ensures code quality standards** are met
- **Builds confidence** in the TDD process through transparency
